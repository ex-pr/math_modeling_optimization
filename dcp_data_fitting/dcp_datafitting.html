<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Daria Dubovskaia">

<title>Homework 5: Disciplined Convex Programming and Data Fitting</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="dcp_datafitting_files/libs/clipboard/clipboard.min.js"></script>
<script src="dcp_datafitting_files/libs/quarto-html/quarto.js"></script>
<script src="dcp_datafitting_files/libs/quarto-html/popper.min.js"></script>
<script src="dcp_datafitting_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="dcp_datafitting_files/libs/quarto-html/anchor.min.js"></script>
<link href="dcp_datafitting_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="dcp_datafitting_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="dcp_datafitting_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="dcp_datafitting_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="dcp_datafitting_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Homework 5: Disciplined Convex Programming and Data Fitting</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Daria Dubovskaia </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="problem-1-penalty-function-approximations-modified-from-exercise-6.4-in-cvx-book-extended-exercises" class="level2">
<h2 class="anchored" data-anchor-id="problem-1-penalty-function-approximations-modified-from-exercise-6.4-in-cvx-book-extended-exercises">Problem 1: Penalty Function Approximations (Modified from Exercise 6.4 in CVX Book Extended Exercises)</h2>
<p>Consider the approximation problem: <span class="math display">\[
\min_{\mathbf{x}\in\mathbb{R}^n} \phi\left(A\mathbf{x}-\mathbf{b}\right),
\]</span> where <span class="math inline">\(A\)</span> is an <span class="math inline">\(m\times n\)</span> matrix, <span class="math inline">\(x\in\mathbb{R}^n\)</span>, and <span class="math inline">\(\phi: \mathbb{R}^m\to\mathbb{R}\)</span> is a convex penalty function measuring the approximation error, and <span class="math inline">\(\mathbf{b}\)</span> is an <span class="math inline">\(m\)</span>-vector.</p>
<p>The purpose of this exercise is for you to implement several different penalty functions in <code>CVX</code> and study how the resulting coefficients <span class="math inline">\(x\)</span> from each penalty function differ, as a means of building intuition about penalty functions.</p>
<p>You will use the following penalty functions:</p>
<ol type="a">
<li><p><span class="math inline">\(\phi(\mathbf{y}) = \|y\|_2\)</span>, the standard Euclidean norm</p></li>
<li><p><span class="math inline">\(\phi(\mathbf{y}) = \|y\|_1\)</span>, the <span class="math inline">\(L_1\)</span> norm. This is often referred to as the Lasso</p></li>
<li><p><span class="math inline">\(\phi(\mathbf{y}) = \sum_{k=1}^{m/2} |y_{r_k}|\)</span>, where <span class="math inline">\(r_k\)</span> is the index of the component with the <span class="math inline">\(k\)</span>th largest absolute value. This is like the Lasso, but where we only count the terms with their error in the top half, i.e.&nbsp;<span class="math inline">\(y_{r_1}\)</span> is the <span class="math inline">\(y\)</span> with largest absolute value, <span class="math inline">\(y_{r_2}\)</span> is the <span class="math inline">\(y\)</span> with second largest absolute value, etc.</p></li>
<li><p><span class="math inline">\(\phi(\mathbf{y}) = \sum_{k=1}^m h(y_k)\)</span>, where <span class="math inline">\(h(y)\)</span> is the Huber penalty, defined by:</p></li>
</ol>
<p><span class="math display">\[
h(u) = \begin{cases} u^2,\, |u|\leq M \\
M(2|u|-M),\, |u|\geq M,
\end{cases}
\]</span> For this problem use <span class="math inline">\(M=0.2\)</span></p>
<ol start="5" type="a">
<li><span class="math inline">\(\phi(\mathbf{y}) = \sum_{k=1}^m h(y_k)\)</span>, where <span class="math inline">\(h\)</span> is the log-barrier penalty, defined by: <span class="math display">\[
h(u) = -\log(1-u^2),\quad \mathbf{dom}(h) = \{u |\quad|u|&lt; 1 \}
\]</span></li>
</ol>
<p>Generate data <span class="math inline">\(A\)</span> and <span class="math inline">\(\mathbf{b}\)</span> as follows:</p>
<ul>
<li><span class="math inline">\(m=200\)</span></li>
<li><span class="math inline">\(n=100\)</span></li>
<li><span class="math inline">\(A_{ij} \sim \mathrm{Normal}(\mu = 0,\sigma = 1)\)</span>, each element normally distributed with mean 0 and standard deviation 1</li>
<li>Intialize <span class="math inline">\(b\)</span> as using a normal distribution of mean <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>, and then normalize <span class="math inline">\(b\)</span> so that all of its entries have absolute value less than <span class="math inline">\(1\)</span> by doing something like:
<ul>
<li><span class="math inline">\(b_i \sim Normal(\mu = 0,\sigma = 1)\)</span></li>
<li>and then: <code>b=b/(1.01 max(abs(b)))</code></li>
</ul></li>
</ul>
<p>This is to make sure the <code>log-barrier</code> function as a non-empty domain.</p>
<p>Visualize the distribution of errors (using a tool like a histogram or density plot) for each of these penalty function formulations and comment on the differences that you observe. Each penalty function prioritizes a errors differently, how do these priorities manifest in the distribution of residuals.</p>
<p>Some hints for selected parts:</p>
<ol type="a">
<li><p>Technically this is a least squares problem, you can solve it using Least-Sqares formula or <code>CVX</code></p></li>
<li><p>Use <code>norm(y,1)</code></p></li>
<li><p>Use <code>norm_largest()</code></p></li>
<li><p>Use <code>huber()</code></p></li>
<li><p>The extended exercises claimed that the <code>log-barrier</code> objective needed to be reformulated to use the geometric mean, but I found that this problem worked perfectly well with a straightforward implementation. I suspect that the <code>CVX</code> software was upgraded to better handle <code>log</code> and <code>exp</code> objecties since this exercise was developed.</p></li>
</ol>
<p><strong>Solution:</strong></p>
<p>In this problem we are going to explore how different penalty functions affect the distribution of residuals in an approximation problem. We want to minimize:</p>
<p><span class="math display">\[
\min_{\mathbf{x}\in\mathbb{R}^n} \phi\left(A\mathbf{x}-\mathbf{b}\right),
\]</span></p>
<p>First, we’ll generate the data using the specified parameters and then implement each penalty function to analyze their effects.</p>
<div id="0a231c80" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Load libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cvxpy <span class="im">as</span> cp</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="77e12cab" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Parameters</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">200</span>  <span class="co">#rows</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span>  <span class="co">#columns</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Generate data</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, (m, n))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>b_raw <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, m)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> b_raw <span class="op">/</span> (<span class="fl">1.01</span> <span class="op">*</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(b_raw)))  <span class="co">#Normalize b</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Residuals for a given solution</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_residuals(x):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A <span class="op">@</span> x <span class="op">-</span> b</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot histogram of residuals</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_residual_distribution(residuals_dict, title<span class="op">=</span><span class="st">"Residual Distributions"</span>):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, residuals <span class="kw">in</span> residuals_dict.items():</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        sns.kdeplot(residuals, label<span class="op">=</span>name)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Residual Value'</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Histograms for more detailed view</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    fig, axs <span class="op">=</span> plt.subplots(<span class="bu">len</span>(residuals_dict), <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span><span class="op">*</span><span class="bu">len</span>(residuals_dict)))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, residuals <span class="kw">in</span> residuals_dict.items():</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        axs[i].hist(residuals, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        axs[i].set_title(<span class="ss">f'Residual Distribution for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        axs[i].set_xlabel(<span class="st">'Residual Value'</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        axs[i].set_ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        axs[i].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co">#Dictionary to store results</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>all_residuals <span class="op">=</span> {}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="a">
<li><span class="math inline">\(\phi(\mathbf{y}) = \|y\|_2\)</span>, the standard Euclidean norm.</li>
</ol>
<p>This creates a distribution of residuals that is approximately Gaussian with a mean close to zero. The residuals are fairly evenly distributed, with few very large or very small values. This approach spreads the error quite evenly across all data points, not prioritizing any specific points to have zero error. The standard deviation of residuals is moderate compared to other methods.</p>
<div id="c98fc0b2" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Euclidean norm</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> solve_euc_norm():</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> cp.Variable(n)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    objective <span class="op">=</span> cp.norm(A <span class="op">@</span> x <span class="op">-</span> b, <span class="dv">2</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    problem <span class="op">=</span> cp.Problem(cp.Minimize(objective))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    problem.solve()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.value</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>x_euc<span class="op">=</span> solve_euc_norm()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>residuals_euc <span class="op">=</span> compute_residuals(x_euc)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>all_residuals[<span class="st">"Euclidean norm"</span>] <span class="op">=</span> residuals_euc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="a">
<li><span class="math inline">\(\phi(\mathbf{y}) = \|y\|_1\)</span>, the <span class="math inline">\(L_1\)</span> norm. This is often referred to as the Lasso.</li>
</ol>
<p>This produces a distribution with many more residuals that are exactly or very close to zero. The L1 norm is known for producing sparse solutions, and this extends to the residuals as well. The distribution has a characteristic peak at zero with long tails. This happens because the L1 norm puts equal weight on all residuals, which encourages some residuals to be exactly zero.</p>
<div id="ee8d3283" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#L1 norm</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> solve_l1_norm():</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> cp.Variable(n)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    objective <span class="op">=</span> cp.norm(A <span class="op">@</span> x <span class="op">-</span> b, <span class="dv">1</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    problem <span class="op">=</span> cp.Problem(cp.Minimize(objective))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    problem.solve()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.value</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>x_l1 <span class="op">=</span> solve_l1_norm()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>residuals_l1 <span class="op">=</span> compute_residuals(x_l1)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>all_residuals[<span class="st">"L1 Norm"</span>] <span class="op">=</span> residuals_l1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="a">
<li><span class="math inline">\(\phi(\mathbf{y}) = \sum_{k=1}^{m/2} |y_{r_k}|\)</span>, where <span class="math inline">\(r_k\)</span> is the index of the component with the <span class="math inline">\(k\)</span>th largest absolute value. This is like the Lasso, but where we only count the terms with their error in the top half, i.e.&nbsp;<span class="math inline">\(y_{r_1}\)</span> is the <span class="math inline">\(y\)</span> with largest absolute value, <span class="math inline">\(y_{r_2}\)</span> is the <span class="math inline">\(y\)</span> with second largest absolute value, etc.</li>
</ol>
<p>This produces a bimodal distribution with a clear distinction between the top half residuals (which are minimized) and the bottom half (which receive less attention). The bottom half of residuals (by magnitude) tend to be very small or zero, while the top half are more evenly distributed. This approach focuses on minimizing the largest errors while potentially allowing many small errors. While the problem statement suggests using norm_largest() (not available), the sum_largest() function with the absolute value of residuals correctly implements the Top Half L1 norm, which sums the k largest absolute values of the residuals.</p>
<div id="15952425" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Top half L1 norm</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> solve_top_half_l1():</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> cp.Variable(n)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    residuals <span class="op">=</span> A <span class="op">@</span> x <span class="op">-</span> b</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    objective <span class="op">=</span> cp.sum_largest(cp.<span class="bu">abs</span>(residuals), m <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># objective = cp.norm_largest(residuals, m // 2, 1) module 'cvxpy' has no attribute 'norm_largest'</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    problem <span class="op">=</span> cp.Problem(cp.Minimize(objective))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    problem.solve()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.value</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>x_top_half <span class="op">=</span> solve_top_half_l1()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>residuals_top_half <span class="op">=</span> compute_residuals(x_top_half)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>all_residuals[<span class="st">"Top Half L1 Norm"</span>] <span class="op">=</span> residuals_top_half</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="a">
<li><span class="math inline">\(\phi(\mathbf{y}) = \sum_{k=1}^m h(y_k)\)</span>, where <span class="math inline">\(h(y)\)</span> is the Huber penalty, defined by:</li>
</ol>
<p><span class="math display">\[
h(u) = \begin{cases} u^2,\, |u|\leq M \\
M(2|u|-M),\, |u|\geq M,
\end{cases}
\]</span> For this problem use <span class="math inline">\(M=0.2\)</span>.</p>
<p>The Huber penalty combines aspects of both L1 and L2 norms. For small residuals (&lt; M=0.2), it behaves like L2 (quadratic), resulting in a smooth Gaussian-like distribution near zero. For large residuals (&gt; M=0.2), it behaves like L1 (linear), leading to longer tails than pure L2. This creates a distribution that doesn’t have as many exact zeros as L1 but is less sensitive to outliers than L2.</p>
<div id="9695ee76" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Huber penalty</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> solve_huber():</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> cp.Variable(n)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    residuals <span class="op">=</span> A <span class="op">@</span> x <span class="op">-</span> b</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    objective <span class="op">=</span> cp.<span class="bu">sum</span>(cp.huber(residuals, M))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    problem <span class="op">=</span> cp.Problem(cp.Minimize(objective))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    problem.solve()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.value</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>x_huber <span class="op">=</span> solve_huber()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>residuals_huber <span class="op">=</span> compute_residuals(x_huber)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>all_residuals[<span class="st">"Huber Penalty (M=0.2)"</span>] <span class="op">=</span> residuals_huber</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="5" type="a">
<li><span class="math inline">\(\phi(\mathbf{y}) = \sum_{k=1}^m h(y_k)\)</span>, where <span class="math inline">\(h\)</span> is the log-barrier penalty, defined by: <span class="math display">\[
h(u) = -\log(1-u^2),\quad \mathbf{dom}(h) = \{u |\quad|u|&lt; 1 \}
\]</span>.</li>
</ol>
<p>This creates a distribution that is strictly bounded (all residuals must have magnitude &lt; 1 due to the domain constraint). It strongly penalizes residuals as they approach the boundary of the domain. The distribution tends to have fewer extremely small residuals compared to L1 but more moderate-sized residuals. It prevents any residual from becoming too large, effectively creating a “barrier” at the bounds.</p>
<div id="1196e226" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Log-barrier penalty</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> solve_log_barrier():</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> cp.Variable(n)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    residuals <span class="op">=</span> A <span class="op">@</span> x <span class="op">-</span> b</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    objective <span class="op">=</span> cp.<span class="bu">sum</span>(<span class="op">-</span>cp.log(<span class="dv">1</span> <span class="op">-</span> cp.square(residuals)))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    problem <span class="op">=</span> cp.Problem(cp.Minimize(objective))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    problem.solve(solver<span class="op">=</span>cp.SCS)  <span class="co"># Use SCS for stability</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.value</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>x_log_barrier <span class="op">=</span> solve_log_barrier()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>residuals_log_barrier <span class="op">=</span> compute_residuals(x_log_barrier)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>all_residuals[<span class="st">"Log-Barrier Penalty"</span>] <span class="op">=</span> residuals_log_barrier</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The L1 norm produced the highest percentage of zero and small residuals (50.00%), confirming its theoretical sparsity-promoting property. The optimization effectively satisfies many constraints exactly, at the expense of larger errors in other constraints. The Euclidean norm created a Gaussian-like distribution with the smallest standard deviation (0.204148), confirming its theoretical property of minimizing the overall energy of the residual vector. The Huber penalty demonstrates its hybrid nature, with a standard deviation (0.217647) between L1 and L2, and a more gradual tail than pure L2, confirming its theoretical design to be less sensitive to outliers. The Log-barrier penalty effectively constrained all residuals within bounds (-0.539368 to 0.438490), demonstrating its barrier property as described in interior-point methods. The Top Half L1 norm created the anticipated selective sparsity pattern, with clear evidence of optimization focusing only on the largest residuals, as would be expected from its formulation.</p>
<p>The different penalty functions also vary in computational efficiency:</p>
<p>The Euclidean norm problem is the most efficient to solve, with a closed-form solution available when using the pseudoinverse.</p>
<p>The L1 norm and Top Half L1 norm can be reformulated as linear programs, which are efficiently solvable but may require more computation than L2 problems.</p>
<p>The Huber penalty requires slightly more computational effort than L2 but remains relatively efficient as it is a convex, differentiable function.</p>
<p>The Log-barrier penalty requires specialized solvers (note our use of SCS solver) and can be more computationally intensive, particularly as residuals approach the boundary of the domain, where the function gradient grows very large.</p>
<p>L1 norm produces the sparsest residuals, with many values exactly zero. Euclidean norm distributes error more evenly, with fewer exact zeros. Top Half L1 creates a “selective sparsity” pattern focusing on the largest errors. Handling of Outliers: Euclidean norm is most sensitive to outliers, as large residuals are squared. L1 norm is more robust to outliers, as it weights all errors linearly. Huber provides a compromise, treating moderate outliers differently from extreme ones. Log-barrier completely prevents extreme outliers by enforcing strict bounds.</p>
<p>Euclidean norm creates more Gaussian-like distributions. L1 creates more Laplacian-like distributions with sharper peaks at zero. Top Half L1 creates bimodal or multimodal distributions. Huber creates distributions with Gaussian centers and Laplacian tails. Log-barrier creates bounded distributions with density increasing toward the boundaries. The choice of penalty function should depend on the specific application and what kind of error distribution is most acceptable for that context.</p>
<p>If having many exactly satisfied constraints is important, L1 norm is preferable. If overall error minimization is the goal without concern for individual constraints, Euclidean norm works well. If protection against outliers is needed, Huber or Log-barrier may be better choices. If focusing on the worst errors is the priority, Top Half L1 norm could be appropriate.</p>
<div id="5efab531" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot the residual distributions</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plot_residual_distribution(all_residuals)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Basic statistics for each residual distribution</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>stats_table <span class="op">=</span> {</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Method"</span>: [],</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Mean"</span>: [],</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Median"</span>: [],</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Std Dev"</span>: [],</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Min"</span>: [],</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Max"</span>: [],</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Zero Residuals (%)"</span>: [],</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Small Residuals (%)"</span>: []  <span class="co"># |r| &lt; 0.01</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, residuals <span class="kw">in</span> all_residuals.items():</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Method"</span>].append(name)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Mean"</span>].append(np.mean(residuals))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Median"</span>].append(np.median(residuals))</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Std Dev"</span>].append(np.std(residuals))</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Min"</span>].append(np.<span class="bu">min</span>(residuals))</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Max"</span>].append(np.<span class="bu">max</span>(residuals))</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Zero Residuals (%)"</span>].append(<span class="dv">100</span> <span class="op">*</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(residuals) <span class="op">&lt;</span> <span class="fl">1e-10</span>) <span class="op">/</span> <span class="bu">len</span>(residuals))</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    stats_table[<span class="st">"Small Residuals (%)"</span>].append(<span class="dv">100</span> <span class="op">*</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(residuals) <span class="op">&lt;</span> <span class="fl">0.01</span>) <span class="op">/</span> <span class="bu">len</span>(residuals))</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Print statistics</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(stats_table[<span class="st">"Method"</span>])):</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Method: </span><span class="sc">{</span>stats_table[<span class="st">'Method'</span>][i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Mean: </span><span class="sc">{</span>stats_table[<span class="st">'Mean'</span>][i]<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Median: </span><span class="sc">{</span>stats_table[<span class="st">'Median'</span>][i]<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Std Dev: </span><span class="sc">{</span>stats_table[<span class="st">'Std Dev'</span>][i]<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Range: [</span><span class="sc">{</span>stats_table[<span class="st">'Min'</span>][i]<span class="sc">:.6f}</span><span class="ss">, </span><span class="sc">{</span>stats_table[<span class="st">'Max'</span>][i]<span class="sc">:.6f}</span><span class="ss">]"</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Zero Residuals: </span><span class="sc">{</span>stats_table[<span class="st">'Zero Residuals (%)'</span>][i]<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Small Residuals: </span><span class="sc">{</span>stats_table[<span class="st">'Small Residuals (%)'</span>][i]<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="dcp_datafitting_files/figure-html/cell-9-output-1.png" width="663" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="dcp_datafitting_files/figure-html/cell-9-output-2.png" width="758" height="1430" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Method: Euclidean norm
  Mean: -0.002882
  Median: -0.008749
  Std Dev: 0.204148
  Range: [-0.604201, 0.458006]
  Zero Residuals: 0.00%
  Small Residuals: 4.00%

Method: L1 Norm
  Mean: 0.007878
  Median: -0.000000
  Std Dev: 0.246697
  Range: [-0.977869, 0.758699]
  Zero Residuals: 6.00%
  Small Residuals: 50.00%

Method: Top Half L1 Norm
  Mean: 0.002451
  Median: 0.005015
  Std Dev: 0.221673
  Range: [-0.930342, 0.695068]
  Zero Residuals: 0.00%
  Small Residuals: 2.50%

Method: Huber Penalty (M=0.2)
  Mean: 0.001140
  Median: -0.002733
  Std Dev: 0.217647
  Range: [-0.856548, 0.756019]
  Zero Residuals: 0.00%
  Small Residuals: 4.00%

Method: Log-Barrier Penalty
  Mean: -0.003582
  Median: -0.010052
  Std Dev: 0.204484
  Range: [-0.539368, 0.438490]
  Zero Residuals: 0.00%
  Small Residuals: 3.50%
</code></pre>
</div>
</div>
</section>
<section id="problem-2-fitting-censored-data-extended-exercises-6.13-in-cvx-book" class="level2">
<h2 class="anchored" data-anchor-id="problem-2-fitting-censored-data-extended-exercises-6.13-in-cvx-book">Problem 2: Fitting Censored Data (Extended Exercises 6.13 in CVX Book)</h2>
<p>In some experiments there are two kinds of measurements or data available: The usual ones, in which you get a number (say), and censored data, in which you don’t get the specific number, but are told something about it, such as a lower bound. A classic example is a study of lifetimes of a set of subjects (say, laboratory mice, devices undergoing reliability testing, or people in a long-term, longitudinal study). For those who have died by the end of data collection, we get the lifetime. For those who have not died by the end of data collection, we do not have the lifetime, but we do have a lower bound, i.e., the length of the study. In statistics, we call this type of data <code>right-censored</code> data, meaning that we do not have the exact values in the right tail of the distribution. The data points that are not present are called the censored data values.</p>
<p>We wish to fit a set of data points, <span class="math inline">\(\left((\mathbf{x}_1,y_1), \cdots, (\mathbf{x}_k,y_k)\right)\)</span>, with <span class="math inline">\(\mathbf{x}_k \in \mathbb{R}^n\)</span> and <span class="math inline">\(y_k\in\mathbb{R}\)</span>, with a linear model of the form <span class="math inline">\(y ≈ \mathbf{c}^T \mathbf{x}\)</span>. The vector <span class="math inline">\(\mathbf{c} \in \mathbf{R}^n\)</span> is the model parameter, which we want to choose. We will use a least-squares criterion, i.e., choose <span class="math inline">\(\mathbf{c}\)</span> to minimize:</p>
<p><span class="math display">\[
J = \sum_{i=1}^k \left(y_i - \mathbf{c}^T\mathbf{x}_i\right)^2
\]</span></p>
<p>Here is the tricky part: some of the values of <span class="math inline">\(y_i\)</span> are censored; for these entries, we have only a (given) lower bound. We will re-order the data so that <span class="math inline">\(y_1 , \cdots , y_m\)</span> are given (i.e., uncensored), while <span class="math inline">\(y_{m+1} , \cdots y_k\)</span> are all censored, i.e., unknown, but larger than D, a given number. All the values of <span class="math inline">\(\mathbf{x}_i\)</span> are known.</p>
<ol type="a">
<li><p>Explain how to find <span class="math inline">\(\mathbf{c}\)</span> (the model parameter) and <span class="math inline">\(y_{m+1} ,\cdots , y_k\)</span> (the censored data values) that minimize <span class="math inline">\(J\)</span>. Hint: should the censored data be variables or parameters?</p></li>
<li><p>Carry out the method of part (a) on the data values in the file <a href="https://github.com/georgehagstrom/DATA609Spring2025/blob/main/website/assignments/labs/labData/censored_dict.json">censored_dict.json</a>. You can process this file in <code>R</code> using <code>fromJSON</code> in the <code>jsonlite</code> package or in python using the <code>json</code> library and:</p></li>
</ol>
<div id="5447a9ea" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'censored_dict.json'</span>, <span class="st">'r'</span>) <span class="im">as</span> fp:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> json.load(fp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Report <span class="math inline">\(\hat{\mathbf{c}}\)</span>, the value of <span class="math inline">\(\mathbf{c}\)</span> found using this method. Also find <span class="math inline">\(\hat{\mathbf{c}_{ls}}\)</span> , the least-squares estimate of <span class="math inline">\(\mathbf{c}\)</span> obtained by simply ignoring the censored data samples, i.e., the least-squares estimate based on the data <span class="math inline">\((\mathbf{x}_1 , y_1 ), \cdots (\mathbf{x}_m , y_m )\)</span>. The data file contains <span class="math inline">\(\mathbf{c}_{\mathrm{true}}\)</span> , the true value of <span class="math inline">\(\mathbf{c}\)</span>, in the vector <span class="math inline">\(\mathbf{c}_{\mathrm{true}}\)</span>. Use this to give the two relative errors:</p>
<p><span class="math display">\[
\frac{\|\mathbf{c}_{\mathrm{true}}- \hat{\mathbf{c}}\|_2^2}{\|\mathbf{c}_{\mathrm{true}}\|_2^2},\quad
\frac{\|\mathbf{c}_{\mathrm{true}}- \hat{\mathbf{c}_{ls}}\|_2^2}{\|\mathbf{c}_{\mathrm{true}}\|_2^2}
\]</span></p>
<p><strong>Solution:</strong></p>
<ol type="a">
<li>Explain how to find <span class="math inline">\(\mathbf{c}\)</span> (the model parameter) and <span class="math inline">\(y_{m+1} ,\cdots , y_k\)</span> (the censored data values) that minimize <span class="math inline">\(J\)</span>. Hint: should the censored data be variables or parameters?</li>
</ol>
<p>The objective function to minimize is below, we are going to split it in two terms. The first term has the uncensored data points (where <span class="math inline">\(y_i\)</span> values are known), and the second term has the censored data points (where <span class="math inline">\(y_i\)</span> values are unknown but have a lower bound of <span class="math inline">\(D\)</span>):</p>
<p><span class="math display">\[
J = \sum_{i=1}^k \left(y_i - \mathbf{c}^T\mathbf{x}_i\right)^2, \\
J = \sum_{i=1}^m \left(y_i - \mathbf{c}^T\mathbf{x}_i\right)^2 + \sum_{i=m+1}^k \left(y_i - \mathbf{c}^T\mathbf{x}_i\right)^2.
\]</span></p>
<p>For the censored data points, we treat the unknown responses as variables (denote them by <span class="math inline">\(z_{m+1}, \ldots, z_k\)</span>) rather than as fixed parameters. Because we don’t know their exact values, we only know they are <span class="math inline">\(\geq D\)</span>. When we treat them as variables, we can find the values that minimize the objective function subject to the constraint that <span class="math inline">\(z_i \geq D\)</span> for <span class="math inline">\(i = m+1, \ldots, k\)</span>. The objective <span class="math inline">\(J\)</span> is convex because it is a sum of squared affine functions (Boyd &amp; Vandenberghe, Section 4.4, p.&nbsp;152). The constraints <span class="math inline">\(z_i \geq D\)</span> are linear, preserving convexity.</p>
<p>As a result, we get <span class="math display">\[
\text{minimize} \ \sum_{i=1}^m \left(y_i - \mathbf{c}^T\mathbf{x}_i\right)^2 + \sum_{i=m+1}^k \left(z_i - \mathbf{c}^T\mathbf{x}_i\right)^2 \\
\text{subject} \ z_i \geq D, i = m+1, \ldots, k,
\]</span></p>
<p>where <span class="math inline">\(c \in \mathbb{R}^n, \ z \in \mathbb{R}^{k-m}\)</span>, the variables in this optimization problem are <span class="math inline">\(\mathbf{c}, z_{m+1}, \ldots ,z_k\)</span> (Boyd &amp; Vandenberghe, 2004, Section 4.2.1, p.137).</p>
<p>We get a convex optimization problem, specifically a quadratic program with linear inequality constraints (Boyd &amp; Vandenberghe, 2004, Section 4.4, p.152). The objective function is convex (quadratic) in both <span class="math inline">\(\mathbf{c}\)</span> and the censored <span class="math inline">\(z_i\)</span> values, and the constraints are linear.</p>
<p>As a result, for the censored data points, the optimal values of <span class="math inline">\(z_i\)</span> will be either:</p>
<ul>
<li><p>Equal to <span class="math inline">\(\mathbf{c}^T\mathbf{x}_i\)</span> if <span class="math inline">\(\mathbf{c}^T\mathbf{x}_i \geq D\)</span> (the fit value is already above the threshold).</p></li>
<li><p>Equal to <span class="math inline">\(D\)</span> if <span class="math inline">\(\mathbf{c}^T\mathbf{x}_i &lt; D\)</span> (the fit value is below the threshold, so we set <span class="math inline">\(z_i\)</span> to the minimum possible value).</p></li>
</ul>
<p>If our model predicts a value above the censoring threshold, we use that value; if it predicts a value below the threshold, we use the threshold itself to minimize the squared error. This is because the cost function forces <span class="math inline">\(z_i\)</span> to be as close as possible to <span class="math inline">\(\mathbf{c}^T\mathbf{x}_i\)</span>, while respecting the constraint. In effect, the optimal <span class="math inline">\(z_i\)</span> is given by <span class="math inline">\(z_i = \text{max} \{ D, \mathbf{c}^T\mathbf{x}_i \}\)</span>. This method treats censored values as optimization variables with constraints, which is better than ignoring them completely. It makes use of all available data, even if it’s incomplete. By including the fact that some responses are above a certain threshold D, we add useful constraints that help estimate the parameters more accurately.</p>
<ol start="2" type="a">
<li>Carry out the method of part (a) on the data values in the file censored_dict.json. Report <span class="math inline">\(\hat{\mathbf{c}}\)</span>, the value of <span class="math inline">\(\mathbf{c}\)</span> found using this method. Also find <span class="math inline">\(\hat{\mathbf{c}_{ls}}\)</span> , the least-squares estimate of <span class="math inline">\(\mathbf{c}\)</span> obtained by simply ignoring the censored data samples.</li>
</ol>
<p>In this problem the data are given in a JSON file with keys:</p>
<p><strong>X</strong>: a list of K=100 samples (each sample is a 20 by 1 vector, with n=20), \</p>
<p><strong>y</strong>: a list of M=25 uncensored response values (each is a scalar stored as a 1-element list), \</p>
<p><span class="math inline">\(D\)</span>: the censoring lower bound (one-element list), \</p>
<p><span class="math inline">\(c_{true}\)</span>: the true parameter vector (20 by 1 vector), \</p>
<p><span class="math inline">\(n\)</span>: the number of features (20), \</p>
<p><span class="math inline">\(K\)</span>: total number of samples (100), \</p>
<p><span class="math inline">\(M\)</span>: number of uncensored samples (25).</p>
<p>The idea is to re-order the data so that the first <span class="math inline">\(M\)</span> responses <span class="math inline">\(y_1 \ldots y_M\)</span> are observed (uncensored) and the remaining <span class="math inline">\(K-M\)</span> responses are censored, meaning we know only that the true response is at least <span class="math inline">\(D\)</span>.</p>
<p>The code below loads the JSON file, extracts the data, splits <strong>X</strong> into the uncensored and censored parts, solves the joint problem for <span class="math inline">\(\hat{\mathbf{c}}\)</span> (and the imputed censored values), and also computes the least-squares estimate <span class="math inline">\(\hat{\mathbf{c}_{ls}}\)</span> using only the uncensored data. Finally, it reports the relative squared errors with respect to <span class="math inline">\(\mathbf{c}_{\mathrm{true}}\)</span>.</p>
<p>We load the JSON file and extract:</p>
<ul>
<li><p><strong>X</strong> as a <span class="math inline">\(K \ \text{by} \ n\)</span> array,</p></li>
<li><p><span class="math inline">\(y_{unc}\)</span> (the uncensored responses) as a vector of length M,</p></li>
<li><p><span class="math inline">\(\mathbf{c}_{\mathrm{true}}\)</span> (the true model parameters),</p></li>
<li><p>The censoring lower bound <span class="math inline">\(D\)</span>.</p></li>
</ul>
<p>We set the first <span class="math inline">\(M\)</span> rows of <strong>X</strong> (i.e.&nbsp;<span class="math inline">\(X_{unc}\)</span>) with known responses <span class="math inline">\(y_{unc}\)</span>. The remaining K-M rows <span class="math inline">\(X_{cens}\)</span> correspond to the censored samples. We define optimization variables <span class="math inline">\(c\)</span> (the n-dimensional model parameter) and <span class="math inline">\(z\)</span> (a vector of length K-M representing the unknown responses for censored data). The objective is the sum of squared errors for the uncensored data plus that for the censored data (using <span class="math inline">\(z\)</span> in place of the unknown responses). We add constraints <span class="math inline">\(z_i \geq D\)</span> for all censored samples. Solving this problem yields an estimate <span class="math inline">\(\hat{\mathbf{c}}\)</span>.</p>
<p>We compute <span class="math inline">\(\hat{\mathbf{c}_{ls}}\)</span> by solving the standard least-squares problem on only the uncensored data (<span class="math inline">\(X_{unc},y_{unc}\)</span>).</p>
<p>We then compute the relative squared errors of both estimates with respect to <span class="math inline">\(\mathbf{c}_{\mathrm{true}}\)</span>.</p>
<p>The results show that the joint estimation method—which treats the censored responses as optimization variables with the constraint <span class="math inline">\(z_i \geq D\)</span> — produces a much more accurate estimate of the true parameter vector than simply ignoring the censored data.</p>
<p>The joint censored-data fit yielded an estimated <span class="math inline">\(\hat{\mathbf{c}}\)</span> with a relative error of approximately 0.017.</p>
<p>In contrast, the least squares estimate using only the uncensored data produced a relative error of approximately 0.111.</p>
<p>The plot below shows a parameter-by-parameter comparison using a bar chart. The joint estimation method (blue bars) produces values closer to the true parameters (green bars) than the least-squares method using only uncensored data (red bars). The difference is particularly pronounced for parameters like indices 0, 5, and 17, where the least-squares estimate shows substantial deviation from the true value. Another plot below presents scatter plots of both estimation methods against the true parameters. The diagonal line represents perfect estimation. The joint estimation method (left panel) shows points clustered much more tightly around this line compared to the least-squares method (right panel), which displays greater scatter and deviation. We can see the superior accuracy of the joint estimation approach, confirming our numerical finding that the relative error is reduced from 0.111 to 0.017 when properly accounting for censored data.</p>
<p>This means that incorporating the censored data (by jointly estimating the imputed values) improves the accuracy of the parameter estimate significantly compared to using only the uncensored data. This result indicates that by jointly estimating the censored responses as decision variables, the model effectively leverages additional information from the censored data. In contrast, ignoring the censored data forces the estimation to rely solely on the uncensored subset, leading to a larger error.</p>
<div id="37da44a5" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Load data</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://raw.githubusercontent.com/georgehagstrom/DATA609Spring2025/refs/heads/main/website/assignments/labs/labData/censored_dict.json'</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>resp <span class="op">=</span> requests.get(url)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> json.loads(resp.text)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Extract parameters from JSON</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> data[<span class="st">'D'</span>][<span class="dv">0</span>]            <span class="co">#censoring lower bound (scalar)</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> data[<span class="st">'K'</span>]               <span class="co">#total number of samples (100)</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> data[<span class="st">'M'</span>]               <span class="co">#number of uncensored samples (25)</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> data[<span class="st">'n'</span>]               <span class="co">#feature dimension (20)</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">#JSON gives a list of 20 arrays (each of length 100)</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">#So transpose to 100 samples each of length 20</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[xi[<span class="dv">0</span>] <span class="cf">for</span> xi <span class="kw">in</span> sample] <span class="cf">for</span> sample <span class="kw">in</span> data[<span class="st">'X'</span>]]).T</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">#y is a list of 25 values (each stored as a one-element list)</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>y_unc <span class="op">=</span> np.array([yi[<span class="dv">0</span>] <span class="cf">for</span> yi <span class="kw">in</span> data[<span class="st">'y'</span>]])  <span class="co">#shape (M,)</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co">#True parameter vector (shape (n,))</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>c_true <span class="op">=</span> np.array([ci[<span class="dv">0</span>] <span class="cf">for</span> ci <span class="kw">in</span> data[<span class="st">'c_true'</span>]])</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">#Split X into uncensored and censored parts</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>X_unc <span class="op">=</span> X[:M, :]    <span class="co">#first M rows (shape (25,20))</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>X_cens <span class="op">=</span> X[M:, :]   <span class="co">#remaining rows (shape (K-M,20))</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="co">#Optimization variables, c (model parameter) and z (imputed censored responses)</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> cp.Variable(n)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> cp.Variable(K <span class="op">-</span> M)  <span class="co">#for samples M+1 ... K</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co">#Objective:</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>obj_unc <span class="op">=</span> cp.sum_squares(y_unc <span class="op">-</span> X_unc <span class="op">@</span> c)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>obj_cens <span class="op">=</span> cp.sum_squares(z <span class="op">-</span> X_cens <span class="op">@</span> c)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>objective <span class="op">=</span> cp.Minimize(obj_unc <span class="op">+</span> obj_cens)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co">#Constraint</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>constraints <span class="op">=</span> [z <span class="op">&gt;=</span> D]</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="co">#Solve</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> cp.Problem(objective, constraints)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>prob.solve()</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>c_hat <span class="op">=</span> c.value  <span class="co">#estimated c using censored data</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a><span class="co">#Least-squares using only uncensored data</span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>c_ls, _, _, _ <span class="op">=</span> np.linalg.lstsq(X_unc, y_unc, rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co">#Relative squared errors with respect to c_true</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>rel_error_c <span class="op">=</span> np.linalg.norm(c_true <span class="op">-</span> c_hat)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> np.linalg.norm(c_true)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>rel_error_ls <span class="op">=</span> np.linalg.norm(c_true <span class="op">-</span> c_ls)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> np.linalg.norm(c_true)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimated c (joint censored-data fit): </span><span class="sc">{</span>c_hat<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimated c (LS using uncensored data only): </span><span class="sc">{</span>c_ls<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Relative error for joint censored-data estimate: </span><span class="sc">{</span>rel_error_c<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Relative error for LS (uncensored only) estimate: </span><span class="sc">{</span>rel_error_ls<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a><span class="co">#Bar chart true parameters vs estimated parameters</span></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>param_idx <span class="op">=</span> np.arange(n)</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>plt.bar(param_idx <span class="op">-</span> width, c_true, width, label<span class="op">=</span><span class="st">'True parameters'</span>, color<span class="op">=</span><span class="st">'green'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>plt.bar(param_idx, c_hat, width, label<span class="op">=</span><span class="st">'Joint estimate (Censored)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>plt.bar(param_idx <span class="op">+</span> width, c_ls, width, label<span class="op">=</span><span class="st">'LS estimate (Uncensored only)'</span>, color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a><span class="co">#Add features</span></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'index'</span>)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'value'</span>)</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'True vs. Estimated Parameters'</span>)</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>plt.xticks(param_idx)</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a><span class="co">#Text box with the relative errors</span></span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>textstr <span class="op">=</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join((</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'Joint estimate relative error: </span><span class="sc">{</span>rel_error_c<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'LS estimate relative error: </span><span class="sc">{</span>rel_error_ls<span class="sc">:.3f}</span><span class="ss">'</span>))</span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>props <span class="op">=</span> <span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, textstr, transform<span class="op">=</span>plt.gca().transAxes, fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>         verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span>props)</span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a><span class="co">#Scatter plot</span></span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a>plt.scatter(c_true, c_hat, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>], <span class="st">'k--'</span>)</span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True parameters'</span>)</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Joint estimate'</span>)</span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Joint estimate vs. True (Error: </span><span class="sc">{</span>rel_error_c<span class="sc">:.3f}</span><span class="ss">)'</span>)</span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'equal'</span>)</span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a>plt.scatter(c_true, c_ls, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>], <span class="st">'k--'</span>)</span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'True parameters'</span>)</span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'LS estimate (Uncensored only)'</span>)</span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'LS estimate vs. True (Error: </span><span class="sc">{</span>rel_error_ls<span class="sc">:.3f}</span><span class="ss">)'</span>)</span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'equal'</span>)</span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated c (joint censored-data fit): [-0.40631053  0.40833081 -0.32259128 -0.64888897  0.34007577 -1.86535897
 -0.9181046  -1.17064612 -0.34459207 -0.46606118 -0.21649319  0.25313271
  0.52413844  0.31509405 -0.50482657  0.58436792 -0.18619886  1.63899706
  0.68224892  0.10731733]
Estimated c (LS using uncensored data only): [-0.86949623  0.38779257 -0.07924954 -0.52689695  0.44848029 -2.1460358
 -0.79172669 -0.86627365 -0.18321402 -0.25059169 -0.15833986  0.55531191
  0.42815852  0.06903265 -0.43355176  0.35683189 -0.20242313  2.00710944
  0.81069272  0.09358954]
Relative error for joint censored-data estimate: 0.017
Relative error for LS (uncensored only) estimate: 0.111</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="dcp_datafitting_files/figure-html/cell-11-output-2.png" width="758" height="566" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="dcp_datafitting_files/figure-html/cell-11-output-3.png" width="758" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="problem-3-robust-logistic-regression-exercise-6.29-in-the-cvx-book-extended-exercises" class="level2">
<h2 class="anchored" data-anchor-id="problem-3-robust-logistic-regression-exercise-6.29-in-the-cvx-book-extended-exercises">Problem 3: Robust Logistic Regression (Exercise 6.29 in the CVX Book extended exercises)</h2>
<p>We are given a data set <span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> , <span class="math inline">\(y_i \in \{−1, 1\}, i = 1, \cdots , n\)</span>. We seek a prediction model <span class="math inline">\(\hat{y} = \mathrm{sign}(\theta^T \mathbf{x})\)</span>, where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> is the model parameter. In logistic regression, <span class="math inline">\(\theta\)</span> is chosen as the minimizer of the logistic loss: <span class="math display">\[
l(\theta) = \sum_{i=1}^n \log\left(1 + \exp\left(-y_i\theta^T\mathbf{x}_i\right)\right)
\]</span></p>
<p>which is a convex function of <span class="math inline">\(\theta\)</span>. Here <span class="math inline">\(\|\delta_i\|_{\infty} = \max_j |\delta_{ij}|\)</span>. Remember that each <span class="math inline">\(\delta_i\)</span> is a vector with length the same as <span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<p>In robust regression, we take into account the idea that the feature vectors <span class="math inline">\(\mathbf{x}_i\)</span> are not known precisely. Specifically we imagine that each entry of each feature vector can vary by <span class="math inline">\(\pm\epsilon\)</span>, where <span class="math inline">\(\epsilon &gt; 0\)</span> is a given uncertainty level. We define the worst-case logistic loss as:</p>
<p><span class="math display">\[
l_{wc}(\theta) = \sum_{i=1}^n \sup_{\|\delta_i\|_{\infty}\leq\epsilon}\log\left(1+\exp\left(-y_i\theta^T\left(\mathbf{x}_i+\delta_i\right)\right)\right)
\]</span></p>
<p>In words: we perturb each feature vector’s entries by up to <span class="math inline">\(\epsilon\)</span> in such a way as to make the logistic loss as large as possible. Each term is convex, since it is the supremum of a family of convex functions of <span class="math inline">\(\theta\)</span>, and so <span class="math inline">\(l_{wc}(\theta)\)</span> is a convex function of <span class="math inline">\(\theta\)</span>.</p>
<p>In robust logistic regression, we choose <span class="math inline">\(\theta\)</span> to minimize <span class="math inline">\(l_{wc}(\theta)\)</span>.</p>
<ol type="a">
<li><p>Explain how to carry out robust logistic regression by solving a single convex optimization problem in disciplined convex programming (DCP) form. Justify any change of variables or introduction of new variables. Explain why solving the problem you propose also solves the robust logistic regression problem. Hint: <span class="math inline">\(log(1 + exp(u)))\)</span> is monotonic in u.</p></li>
<li><p>Fit a standard logistic regression model (i.e., minimize <span class="math inline">\(l(\theta)\)</span>), and also a robust logistic regression model (i.e., minimize <span class="math inline">\(l_{wc}(\theta)\)</span>), using the data given in <a href="https://github.com/georgehagstrom/DATA609Spring2025/blob/main/website/assignments/labs/labData/rob_regression.csv">rob_regression.csv</a> and <a href="https://github.com/georgehagstrom/DATA609Spring2025/blob/main/website/assignments/labs/labData/rob_regression_test.csv">rob_regression_test.csv</a>. The <span class="math inline">\(\mathbf{x}_i\)</span>s are provided as the rows of an <span class="math inline">\(n \times d\)</span> matrix named <span class="math inline">\(X\)</span> (these are the variables of the data frame named “X_1, X_2, …”). The <span class="math inline">\(y_i\)</span>s are provided as the entries of a <span class="math inline">\(n\)</span>-vector named <span class="math inline">\(y\)</span> (the first column in the data frame). The file also contains a test data set, <span class="math inline">\(X_{\mathrm{test}}\)</span>, <span class="math inline">\(y_{\mathrm{test}}\)</span>. Give the test error rate (i.e., fraction of test set data points for which <span class="math inline">\(\hat{y}= y\)</span>) for the logistic regression and robust logistic regression models.</p></li>
</ol>
<p><strong>Solution:</strong></p>
<ol type="a">
<li>The worst-case logistic loss is given by:</li>
</ol>
<p><span class="math display">\[
l_{wc}(\theta) = \sum_{i=1}^n \sup_{\|\delta_i\|_{\infty}\leq\epsilon}\log\left(1+\exp\left(-y_i\theta^T\left(\mathbf{x}_i+\delta_i\right)\right)\right)
\]</span>,</p>
<p>where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> is the model parameter, <span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> are the feature vectors, <span class="math inline">\(y_i \in \{−1, 1\}, i = 1, \cdots , n\)</span> are the labels, <span class="math inline">\(\|\delta_i\|_{\infty} = \max_j |\delta_{ij}| \leq \epsilon\)</span> is the uncertainty in each feature.</p>
<p>The log term is monotonically increasing with the expression inside, which means the supremum will occur when the exponent <span class="math inline">\(-y_i\theta^T(\mathbf{x}_i+\delta_i)\)</span> is maximized over <span class="math inline">\(\|\delta_i\|_{\infty} \leq \epsilon\)</span>. Since <span class="math inline">\(-y_i\theta^T\mathbf{x}_i\)</span> is fixed for a given <span class="math inline">\(\theta\)</span>, we focus on maximizing <span class="math inline">\(-y_i\theta^T\delta_i\)</span>.</p>
<p>For this linear function <span class="math inline">\(-y_i\theta^T\delta_i\)</span> subject to <span class="math inline">\(\|\delta_i\|_{\infty} \leq \epsilon\)</span>, the maximum occurs when each component: <span class="math inline">\(\delta_{ij​} = \epsilon ⋅ \mathrm{sign} (-y_i \theta_j)\)</span>. Thus, substituting these optimal perturbations into the loss:</p>
<p><span class="math display">\[
-y_i\theta^T\delta_i = \sum_{j=1}^d (-y_i \theta_j) (\epsilon ⋅ \mathrm{sign} (-y_i \theta_j)) = \epsilon \sum_{j=1}^d |\theta_j| = \epsilon \|\theta\|_1,
\]</span></p>
<p>which is the expression used in our robust loss. Since <span class="math inline">\(y_i \in \{−1, 1\}, i = 1, \cdots , n\)</span> and <span class="math inline">\(-y_i \theta_j  ⋅ \mathrm{sign} (-y_i \theta_j) = |-y_i \theta_j| = |\theta_j|\)</span>, the worst-case logistic loss simplifies to:</p>
<p><span class="math display">\[l_{wc}(\theta) = \sum_{i=1}^n \log\left(1+\exp\left(-y_i\theta^T\mathbf{x}_i + \epsilon\|\theta\|_1\right)\right)\]</span>.</p>
<p>This is a convex function of <span class="math inline">\(\theta\)</span> as it is a composition of the convex log-sum-exp function with an affine mapping (Boyd &amp; Vandenberghe, 2004, Section 3.1.5, p.&nbsp;75 and Section 3.2.4, p.&nbsp;84).</p>
<p>To express this in DCP form, we need to handle the <span class="math inline">\(\ell_1\)</span> norm <span class="math inline">\(\|\theta\|_1 = \sum_{j=1}^d |\theta_j|\)</span> (which is convex). We introduce auxiliary variables to linearize the absolute values (Boyd &amp; Vandenberghe, 2004, Section 4.1.3, p.&nbsp;130): Variable <span class="math inline">\(t \in \mathbb{R}\)</span> to represent <span class="math inline">\(\epsilon\|\theta\|_1\)</span>; variables <span class="math inline">\(u_j \geq |\theta_j|\)</span> (Boyd &amp; Vandenberghe, 2004, Section 6.2.3, p.&nbsp;320).</p>
<p>The problem becomes:</p>
<p><span class="math display">\[\begin{align}
\text{minimize} \quad &amp; \sum_{i=1}^n \log\left(1+\exp\left(-y_i\theta^T\mathbf{x}_i + t\right)\right) \\
\text{subject to} \quad &amp; t \geq \epsilon \sum_{j=1}^d u_j \\
&amp; u_j \geq \theta_j, \ u_j \geq -\theta_j \quad j = 1, \ldots, d \\
\end{align},
\]</span></p>
<p>where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span>, <span class="math inline">\(t \in  \mathbb{R}\)</span>, <span class="math inline">\(u \in \mathbb{R}^d\)</span> are variables.</p>
<p>This formulation is a sum of log-sum-exp terms, which are convex. The constraints are linear, hence convex (Boyd &amp; Vandenberghe, 2004, Section 4.2, p.&nbsp;136). At the optimum <span class="math inline">\(u_j = |\theta_j|\)</span>, because the objective is increasing in <span class="math inline">\(t\)</span>, and <span class="math inline">\(t= \epsilon \sum_{j=1}^d u_j= \epsilon\|\theta\|_1\)</span> due to the minimization (Boyd &amp; Vandenberghe, 2004, Section 4.1.3, p.130, Section 4.2.4 p.142). Since the <span class="math inline">\(\log\)</span> function is convex and nondecreasing, and <span class="math inline">\(t\)</span> is an affine function of <span class="math inline">\(u\)</span>, the overall objective is convex in <span class="math inline">\(\theta\)</span> (Boyd &amp; Vandenberghe, 2004, Section 4.2, p.136). Thus, the objective matches <span class="math inline">\(l_{wc}(\theta)\)</span></p>
<p>The objective becomes exactly the worst-case logistic loss, it directly minimizes <span class="math inline">\(l_{wc}(\theta)\)</span> in a convex, DCP-compatible form, solving the robust logistic regression problem.</p>
<ol start="2" type="a">
<li>Results:</li>
</ol>
<p>Standard Logistic Regression Error Rate: 0.4.</p>
<p>Robust Logistic Regression Error Rate: 0.2.</p>
<p>L1 norm of standard model parameters: 311.820</p>
<p>L1 norm of robust model parameters: 7837.347</p>
<p>The robust model has a significantly lower error rate on the test set, confirming that accounting for potential perturbations in the features improves generalization performance. The robust model has a much larger L1 norm (approximately 25 times larger) than the standard model. The large L1 norm in the robust model can be understood by examining the formulation: while the epsilon term does create an L1 regularization-like effect, the primary goal of the optimization is to find parameters that are robust to worst-case perturbations. In this case, that leads to larger parameter values that can withstand perturbations while still maintaining good classification performance.</p>
<p>The models disagree on 14 out of 60 test points (23.3% of the test data). Of these disagreement points, the robust model correctly classifies 13 points that the standard model misclassifies, while only 1 point is correctly classified by the standard model but misclassified by the robust model. This demonstrates that the robust model’s improved performance comes from its ability to handle challenging data points. Features 12, 25, 1, 30, and 31 have the highest standard deviation in these points. Feature 12 has the highest variability with a standard deviation of 1.456. This indicates that the robust model performs particularly well on data points with higher feature variability, which are precisely the cases where feature perturbations would have the most impact.</p>
<p>The robust model’s parameters have much larger magnitudes compared to the standard model across all dimensions, as clearly shown in the parameter comparison visualization. This confirms that the robust model is making stronger predictions to overcome potential adversarial perturbations in the data. Robust optimization sometimes leads to solutions that appear to be “over-compensating” for potential perturbations (Boyd &amp; Vandenberghe, 2004, Section 6.4.2). The robust model is making stronger predictions to ensure that even with feature perturbations of magnitude epsilon, the correct classification is maintained.</p>
<p>The robust logistic regression approach outperforms standard logistic regression on this dataset by explicitly accounting for potential variations in the feature vectors. The performance improvement comes at the cost of a model with larger parameter values, which is a reasonable trade-off when dealing with data that may contain uncertainty or noise in the feature measurements. This approach would be particularly valuable in applications where feature perturbations are likely or where the cost of misclassification is high.</p>
<div id="0b1f64a3" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Load dataset from Github</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">'https://media.githubusercontent.com/media/georgehagstrom/DATA609Spring2025/refs/heads/main/website/assignments/labs/labData/rob_regression.csv'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">'https://media.githubusercontent.com/media/georgehagstrom/DATA609Spring2025/refs/heads/main/website/assignments/labs/labData/rob_regression_test.csv'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>train_df.info(), test_df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 60 entries, 0 to 59
Data columns (total 41 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   y       60 non-null     float64
 1   X_1     60 non-null     float64
 2   X_2     60 non-null     float64
 3   X_3     60 non-null     float64
 4   X_4     60 non-null     float64
 5   X_5     60 non-null     float64
 6   X_6     60 non-null     float64
 7   X_7     60 non-null     float64
 8   X_8     60 non-null     float64
 9   X_9     60 non-null     float64
 10  X_10    60 non-null     float64
 11  X_11    60 non-null     float64
 12  X_12    60 non-null     float64
 13  X_13    60 non-null     float64
 14  X_14    60 non-null     float64
 15  X_15    60 non-null     float64
 16  X_16    60 non-null     float64
 17  X_17    60 non-null     float64
 18  X_18    60 non-null     float64
 19  X_19    60 non-null     float64
 20  X_20    60 non-null     float64
 21  X_21    60 non-null     float64
 22  X_22    60 non-null     float64
 23  X_23    60 non-null     float64
 24  X_24    60 non-null     float64
 25  X_25    60 non-null     float64
 26  X_26    60 non-null     float64
 27  X_27    60 non-null     float64
 28  X_28    60 non-null     float64
 29  X_29    60 non-null     float64
 30  X_30    60 non-null     float64
 31  X_31    60 non-null     float64
 32  X_32    60 non-null     float64
 33  X_33    60 non-null     float64
 34  X_34    60 non-null     float64
 35  X_35    60 non-null     float64
 36  X_36    60 non-null     float64
 37  X_37    60 non-null     float64
 38  X_38    60 non-null     float64
 39  X_39    60 non-null     float64
 40  X_40    60 non-null     float64
dtypes: float64(41)
memory usage: 19.7 KB
&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 60 entries, 0 to 59
Data columns (total 41 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   y_test     60 non-null     float64
 1   X_test_1   60 non-null     float64
 2   X_test_2   60 non-null     float64
 3   X_test_3   60 non-null     float64
 4   X_test_4   60 non-null     float64
 5   X_test_5   60 non-null     float64
 6   X_test_6   60 non-null     float64
 7   X_test_7   60 non-null     float64
 8   X_test_8   60 non-null     float64
 9   X_test_9   60 non-null     float64
 10  X_test_10  60 non-null     float64
 11  X_test_11  60 non-null     float64
 12  X_test_12  60 non-null     float64
 13  X_test_13  60 non-null     float64
 14  X_test_14  60 non-null     float64
 15  X_test_15  60 non-null     float64
 16  X_test_16  60 non-null     float64
 17  X_test_17  60 non-null     float64
 18  X_test_18  60 non-null     float64
 19  X_test_19  60 non-null     float64
 20  X_test_20  60 non-null     float64
 21  X_test_21  60 non-null     float64
 22  X_test_22  60 non-null     float64
 23  X_test_23  60 non-null     float64
 24  X_test_24  60 non-null     float64
 25  X_test_25  60 non-null     float64
 26  X_test_26  60 non-null     float64
 27  X_test_27  60 non-null     float64
 28  X_test_28  60 non-null     float64
 29  X_test_29  60 non-null     float64
 30  X_test_30  60 non-null     float64
 31  X_test_31  60 non-null     float64
 32  X_test_32  60 non-null     float64
 33  X_test_33  60 non-null     float64
 34  X_test_34  60 non-null     float64
 35  X_test_35  60 non-null     float64
 36  X_test_36  60 non-null     float64
 37  X_test_37  60 non-null     float64
 38  X_test_38  60 non-null     float64
 39  X_test_39  60 non-null     float64
 40  X_test_40  60 non-null     float64
dtypes: float64(41)
memory usage: 19.7 KB</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>(None, None)</code></pre>
</div>
</div>
<div id="0fe429e9" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Extract features X and target y</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">'y'</span>].values</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df.drop(columns<span class="op">=</span>[<span class="st">'y'</span>]).values</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test_df[<span class="st">'y_test'</span>].values</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_df.drop(columns<span class="op">=</span>[<span class="st">'y_test'</span>]).values</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Dimensions</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>n, d <span class="op">=</span> X_train.shape</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Standard logistic regression</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>theta_std <span class="op">=</span> cp.Variable(d)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>objective_std <span class="op">=</span> cp.<span class="bu">sum</span>(cp.logistic(<span class="op">-</span>cp.multiply(y_train, X_train <span class="op">@</span> theta_std)))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>problem_std <span class="op">=</span> cp.Problem(cp.Minimize(objective_std))</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>problem_std.solve()</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>theta_std_val <span class="op">=</span> theta_std.value</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">#Robust logistic regression</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>theta_rob <span class="op">=</span> cp.Variable(d)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> cp.Variable()</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> cp.Variable(d)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co">#Objective with worst-case perturbation</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>objective_terms <span class="op">=</span> []</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    term <span class="op">=</span> cp.logistic(<span class="op">-</span>(y_train[i] <span class="op">*</span> (X_train[i] <span class="op">@</span> theta_rob)) <span class="op">+</span> t)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    objective_terms.append(term)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>objective_rob <span class="op">=</span> <span class="bu">sum</span>(objective_terms)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co">#Constraints: t = epsilon * L1 norm of theta</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>constraints <span class="op">=</span> [</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    t <span class="op">==</span> epsilon <span class="op">*</span> cp.<span class="bu">sum</span>(u),</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    u <span class="op">&gt;=</span> theta_rob,</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    u <span class="op">&gt;=</span> <span class="op">-</span>theta_rob</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="co">#OR</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="co">#loss_rob = cp.sum(cp.logistic(-cp.multiply(y_train, X_train @ theta_rob)</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="co">#+ epsilon * cp.norm(theta_rob, 1)))</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>problem_rob <span class="op">=</span> cp.Problem(cp.Minimize(objective_rob), constraints)</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>problem_rob.solve(verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>theta_rob_val <span class="op">=</span> theta_rob.value</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="co">#Predictions</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>y_pred_std <span class="op">=</span> np.sign(X_test <span class="op">@</span> theta_std_val)</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>y_pred_rob <span class="op">=</span> np.sign(X_test <span class="op">@</span> theta_rob_val)</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a><span class="co">#Test error rates</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>error_rate_std <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> accuracy_score(y_test, y_pred_std)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>error_rate_rob <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> accuracy_score(y_test, y_pred_rob)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Standard logistic regression error rate: </span><span class="sc">{</span>error_rate_std<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Robust logistic regression error rate: </span><span class="sc">{</span>error_rate_rob<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a><span class="co">#L1 norms</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>l1_std <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(theta_std_val))</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>l1_rob <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(theta_rob_val))</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"L1 norm, standard model: </span><span class="sc">{</span>l1_std<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"L1 norm, robust model: </span><span class="sc">{</span>l1_rob<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a><span class="co">#Identify points where models disagree</span></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>disagree_id <span class="op">=</span> np.where(y_pred_std <span class="op">!=</span> y_pred_rob)[<span class="dv">0</span>]</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>correct_robust_id <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> disagree_id <span class="cf">if</span> y_pred_rob[i] <span class="op">==</span> y_test[i]]</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of test points where models disagree: </span><span class="sc">{</span><span class="bu">len</span>(disagree_id)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of points where robust model is correct but standard model is wrong: </span><span class="sc">{</span><span class="bu">len</span>(correct_robust_id)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="co">#Analyze feature characteristics of these points</span></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(correct_robust_id) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>    correct_robust_features <span class="op">=</span> X_test[correct_robust_id]</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>    feature_means <span class="op">=</span> np.mean(correct_robust_features, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>    feature_stds <span class="op">=</span> np.std(correct_robust_features, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Top 5 features with highest variability</span></span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>    top_variable_features <span class="op">=</span> np.argsort(feature_stds)[<span class="op">-</span><span class="dv">5</span>:]</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Top 5 features with highest variability in the points where robust model outperforms standard model:"</span>)</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> top_variable_features:</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Feature </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: Mean = </span><span class="sc">{</span>feature_means[idx]<span class="sc">:.3f}</span><span class="ss">, Std = </span><span class="sc">{</span>feature_stds[idx]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a><span class="co">#Visualization of parameter values</span></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(d), np.<span class="bu">abs</span>(theta_std_val), alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Standard Model'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(d), np.<span class="bu">abs</span>(theta_rob_val), alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Robust Model'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Index'</span>)</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Absolute Parameter Value'</span>)</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparison of Parameter Magnitudes'</span>)</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>===============================================================================
                                     CVXPY                                     
                                     v1.6.2                                    
===============================================================================
(CVXPY) Apr 06 03:39:01 PM: Your problem has 81 variables, 81 constraints, and 0 parameters.
(CVXPY) Apr 06 03:39:01 PM: It is compliant with the following grammars: DCP, DQCP
(CVXPY) Apr 06 03:39:01 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)
(CVXPY) Apr 06 03:39:01 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.
(CVXPY) Apr 06 03:39:01 PM: Your problem is compiled with the CPP canonicalization backend.
-------------------------------------------------------------------------------
                                  Compilation                                  
-------------------------------------------------------------------------------
(CVXPY) Apr 06 03:39:01 PM: Compiling problem (target solver=CLARABEL).
(CVXPY) Apr 06 03:39:01 PM: Reduction chain: Dcp2Cone -&gt; CvxAttr2Constr -&gt; ConeMatrixStuffing -&gt; CLARABEL
(CVXPY) Apr 06 03:39:01 PM: Applying reduction Dcp2Cone
(CVXPY) Apr 06 03:39:01 PM: Applying reduction CvxAttr2Constr
(CVXPY) Apr 06 03:39:01 PM: Applying reduction ConeMatrixStuffing
(CVXPY) Apr 06 03:39:01 PM: Applying reduction CLARABEL
(CVXPY) Apr 06 03:39:01 PM: Finished problem compilation (took 1.128e-01 seconds).
-------------------------------------------------------------------------------
                                Numerical solver                               
-------------------------------------------------------------------------------
(CVXPY) Apr 06 03:39:01 PM: Invoking solver CLARABEL  to obtain a solution.
-------------------------------------------------------------
           Clarabel.rs v0.10.0  -  Clever Acronym                

                   (c) Paul Goulart                          
                University of Oxford, 2022                   
-------------------------------------------------------------

problem:
  variables     = 261
  constraints   = 501
  nnz(P)        = 0
  nnz(A)        = 3021
  cones (total) = 122
    :        Zero = 1,  numel = 1
    : Nonnegative = 1,  numel = 140
    : Exponential = 120,  numel = (3,3,3,3,...,3)

settings:
  linear algebra: direct / qdldl, precision: 64 bit
  max iter = 200, time limit = Inf,  max step = 0.990
  tol_feas = 1.0e-8, tol_gap_abs = 1.0e-8, tol_gap_rel = 1.0e-8,
  static reg : on, ϵ1 = 1.0e-8, ϵ2 = 4.9e-32
  dynamic reg: on, ϵ = 1.0e-13, δ = 2.0e-7
  iter refine: on, reltol = 1.0e-13, abstol = 1.0e-12,
               max iter = 10, stop ratio = 5.0
  equilibrate: on, min_scale = 1.0e-4, max_scale = 1.0e4
               max iter = 10

iter    pcost        dcost       gap       pres      dres      k/t        μ       step      
---------------------------------------------------------------------------------------------
  0  +0.0000e+00  -1.2327e+02  1.23e+02  8.84e-01  4.08e+00  1.00e+00  1.00e+00   ------   
  1  +4.7392e+01  +1.4682e+00  3.13e+01  2.44e-01  9.81e-01  5.46e-01  2.82e-01  7.53e-01  
  2  +2.7267e+01  +1.0389e+01  1.62e+00  7.59e-02  3.79e-01  1.77e-01  1.05e-01  7.92e-01  
  3  +2.1350e+01  +1.6075e+01  3.28e-01  1.90e-02  1.17e-01  5.90e-02  3.14e-02  7.92e-01  
  4  +1.8895e+01  +1.6028e+01  1.79e-01  7.87e-03  6.03e-02  3.48e-02  1.56e-02  6.34e-01  
  5  +1.7121e+01  +1.5494e+01  1.05e-01  3.23e-03  3.11e-02  2.22e-02  7.76e-03  6.34e-01  
  6  +1.5853e+01  +1.4740e+01  7.56e-02  1.48e-03  1.81e-02  1.77e-02  4.34e-03  6.34e-01  
  7  +1.4705e+01  +1.3885e+01  5.91e-02  6.53e-04  9.97e-03  1.66e-02  2.33e-03  7.74e-01  
  8  +1.3438e+01  +1.2807e+01  4.93e-02  2.88e-04  5.14e-03  1.80e-02  1.19e-03  7.63e-01  
  9  +1.2919e+01  +1.2365e+01  4.48e-02  1.96e-04  3.69e-03  1.88e-02  8.54e-04  7.92e-01  
 10  +1.0482e+01  +1.0111e+01  3.67e-02  8.02e-05  1.63e-03  1.92e-02  3.76e-04  6.34e-01  
 11  +7.4680e+00  +7.2646e+00  2.80e-02  2.85e-05  6.03e-04  1.68e-02  1.39e-04  7.92e-01  
 12  +5.0829e+00  +4.9706e+00  2.26e-02  1.23e-05  2.65e-04  1.16e-02  6.13e-05  6.34e-01  
 13  +3.4825e+00  +3.4198e+00  1.83e-02  5.46e-06  1.19e-04  9.05e-03  2.75e-05  7.92e-01  
 14  +1.5154e+00  +1.4969e+00  1.23e-02  1.32e-06  2.91e-05  3.80e-03  6.74e-06  7.92e-01  
 15  +1.1647e+00  +1.1509e+00  1.21e-02  8.52e-07  1.88e-05  3.49e-03  4.36e-06  5.07e-01  
 16  +4.9929e-01  +4.9467e-01  4.62e-03  2.55e-07  5.65e-06  1.51e-03  1.31e-06  7.92e-01  
 17  +2.0876e-01  +2.0726e-01  1.50e-03  7.62e-08  1.70e-06  6.45e-04  3.93e-07  7.92e-01  
 18  +8.4320e-02  +8.3847e-02  4.72e-04  2.28e-08  5.09e-07  2.69e-04  1.18e-07  7.92e-01  
 19  +3.8446e-02  +3.8264e-02  1.82e-04  8.44e-09  1.89e-07  1.32e-04  4.37e-08  7.92e-01  
 20  +1.4638e-02  +1.4584e-02  5.45e-05  2.53e-09  5.67e-08  4.96e-05  1.31e-08  7.92e-01  
 21  +6.4695e-03  +6.4495e-03  2.00e-05  9.37e-10  2.10e-08  2.28e-05  4.87e-09  7.92e-01  
 22  +2.3737e-03  +2.3680e-03  5.71e-06  2.81e-10  6.32e-09  8.36e-06  1.46e-09  7.92e-01  
 23  +8.5462e-04  +8.5307e-04  1.55e-06  8.43e-11  1.90e-09  3.09e-06  4.38e-10  7.92e-01  
 24  +4.2144e-04  +4.2079e-04  6.51e-07  3.71e-11  8.36e-10  1.53e-06  1.93e-10  6.34e-01  
 25  +3.2591e-04  +3.2543e-04  4.80e-07  2.76e-11  6.21e-10  1.19e-06  1.43e-10  3.24e-01  
 26  +1.1385e-04  +1.1373e-04  1.21e-07  8.27e-12  1.86e-10  4.16e-07  4.30e-11  7.92e-01  
 27  +8.7871e-05  +8.7784e-05  8.73e-08  6.14e-12  1.38e-10  3.22e-07  3.19e-11  3.24e-01  
 28  +5.2026e-05  +5.1981e-05  4.47e-08  3.39e-12  7.64e-11  1.92e-07  1.76e-11  5.07e-01  
 29  +2.8897e-05  +2.8876e-05  2.10e-08  1.75e-12  3.95e-11  1.06e-07  9.09e-12  5.07e-01  
 30  +2.3108e-05  +2.3092e-05  1.53e-08  1.37e-12  3.06e-11  8.52e-08  7.04e-12  3.24e-01  
 31  +1.7752e-05  +1.7741e-05  1.08e-08  9.05e-13  2.22e-11  6.54e-08  5.22e-12  3.24e-01  
 32  +1.0394e-05  +1.0389e-05  5.08e-09  4.89e-13  1.23e-11  3.85e-08  2.88e-12  5.07e-01  
---------------------------------------------------------------------------------------------
Terminated with status = Solved
solve time = 9.5677ms
-------------------------------------------------------------------------------
                                    Summary                                    
-------------------------------------------------------------------------------
(CVXPY) Apr 06 03:39:01 PM: Problem status: optimal
(CVXPY) Apr 06 03:39:01 PM: Optimal value: 2.116e-07
(CVXPY) Apr 06 03:39:01 PM: Compilation took 1.128e-01 seconds
(CVXPY) Apr 06 03:39:01 PM: Solver (including time spent in interface) took 1.145e-02 seconds
Standard logistic regression error rate: 0.400
Robust logistic regression error rate: 0.200
L1 norm, standard model: 311.820
L1 norm, robust model: 7837.947
Number of test points where models disagree: 14
Number of points where robust model is correct but standard model is wrong: 13
Top 5 features with highest variability in the points where robust model outperforms standard model:
Feature 31: Mean = -0.142, Std = 1.081
Feature 30: Mean = -0.197, Std = 1.088
Feature 1: Mean = -0.104, Std = 1.143
Feature 25: Mean = -0.300, Std = 1.221
Feature 12: Mean = 0.219, Std = 1.456</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\daria\AppData\Local\Programs\Python\Python313\Lib\site-packages\cvxpy\problems\problem.py:1481: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="dcp_datafitting_files/figure-html/cell-13-output-3.png" width="673" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<ol type="1">
<li><p>Boyd, S., &amp; Vandenberghe, L. (2004). <em>Convex optimization</em>. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511804441" class="uri">https://doi.org/10.1017/CBO9780511804441</a></p></li>
<li><p>Disciplined Convex Programming -. (n.d.). <a href="https://www.cvxpy.org/tutorial/dcp/index.html" class="uri">https://www.cvxpy.org/tutorial/dcp/index.html</a></p></li>
</ol>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>